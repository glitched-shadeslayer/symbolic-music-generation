{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2f965e-1c7b-4827-948e-012dc07bf072",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e0304-69eb-49ba-8add-2398d0246a22",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84639806-a39c-4e5b-905b-8fcb671b498d",
   "metadata": {},
   "source": [
    "This project focuses on symbolic music harmonization, a task in the field of music information retrieval (MIR) and AI-generated music. The goal is to predict a sequence of chords that complement a given melody, using symbolic representations of music—typically notes and chords encoded from MIDI files. This falls under the broader umbrella of conditioned generation, where one musical element (melody) is used to condition the generation of another (harmony)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f849e3-bedc-427d-9430-9524777fac60",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd644796-0331-4654-9169-e7ff482d1630",
   "metadata": {},
   "source": [
    "Given a sequence of musical notes (melody), automatically generate a corresponding sequence of chords (harmony) that musically complements the melody. This mirrors tasks a human composer might perform and is especially useful for music composition assistance, educational tools, and interactive music applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f422b12-456f-4af7-90c6-95bd377ee8d2",
   "metadata": {},
   "source": [
    "## Symbolic Conditioned Generation & Harmoization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b0953-c282-4993-876a-dd836a6291bd",
   "metadata": {},
   "source": [
    "Symbolic refers to data formats like MIDI, which encode music as discrete symbolic events (notes, durations, velocities) rather than audio waveforms. Unlike audio signals, symbolic representations allow for structured and interpretable data manipulation. The model generates chord sequences conditioned on input melodies, creating musically relevant outputs based on specific inputs. This makes the task similar to machine translation in NLP, where an input sentence (melody) is \"translated\" into another sequence (chords). Harmonization is the process of adding chords to a melody according to music theory principles (e.g., chord functions, voice leading) - creating a fuller musical texture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc96dace-64e3-42a0-b26e-50e7a095396f",
   "metadata": {},
   "source": [
    "## Dataset: Nottingham MIDI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98018308-3ff8-47d1-b576-c842508d13ae",
   "metadata": {},
   "source": [
    "Source:\n",
    "Collection of 1,200+ folk tunes in MIDI format from the Nottingham Music Database. The dataset contains folk tunes each with a melody and a corresponding harmony. \n",
    "\n",
    "Characteristics:\n",
    "1. Primarily monophonic melodies with chord annotations\n",
    "2. Contains folk, dance, and ballad styles\n",
    "3. Typical structure: 8-32 bar phrases in 4/4 time\n",
    "\n",
    "Each file is parsed to extract:\n",
    "1. Melody: A sequence of notes or rests.\n",
    "2. Chords: Either actual chords from the file or placeholder labels (in this code, placeholder \"C\" is used).\n",
    "\n",
    "Format: Standard MIDI files, parsed using music21\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee749dd6-b697-4bd0-a4cc-60490f1bf4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: music21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (9.7.0)\n",
      "Requirement already satisfied: chardet in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from music21) (5.2.0)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from music21) (1.4.2)\n",
      "Requirement already satisfied: jsonpickle in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from music21) (4.1.1)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from music21) (3.9.3)\n",
      "Requirement already satisfied: more-itertools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from music21) (10.7.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from music21) (1.26.4)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from music21) (2.32.3)\n",
      "Requirement already satisfied: webcolors>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from music21) (24.11.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->music21) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->music21) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->music21) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->music21) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sriraksharajeshrao/Library/Python/3.12/lib/python/site-packages (from matplotlib->music21) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->music21) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->music21) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sriraksharajeshrao/Library/Python/3.12/lib/python/site-packages (from matplotlib->music21) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->music21) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->music21) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->music21) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->music21) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sriraksharajeshrao/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->music21) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c4b432-1b55-4054-bc12-6d0451832132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c357d4-d46b-44f0-8d1f-bd019b5af5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"nottingham-dataset-master/MIDI/\"  # Update to your MIDI folder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Sets data path and computation device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352af33-8e8d-480a-a321-a799cfa59cc2",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290876d-403a-4fae-9d6f-4e8c64170ba2",
   "metadata": {},
   "source": [
    "File Parsing: Extract notes and chords using music21\n",
    "\n",
    "Simplification:\n",
    "\n",
    "1. Melodies: Convert to pitch strings (e.g., \"C4\") or \"Rest\"\n",
    "\n",
    "2. Chords: Simplified to root note names (original code uses placeholder)\n",
    "\n",
    "Sequence Alignment: Ensure 1:1 correspondence between notes and chords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e442af8-0b77-48d4-b43b-e846377adad2",
   "metadata": {},
   "source": [
    "### MIDI Parsing\n",
    "- Uses music21 to parse MIDI files.\n",
    "- Extracts melodies and (placeholder) chords from each file.\n",
    "- Converts musical elements to string-based symbolic representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b59bd39-7a8c-4844-82ba-d70a46921039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# MIDI Parser\n",
    "# -----------------------------\n",
    "def parse_midi_file(file_path):\n",
    "    midi = converter.parse(file_path)\n",
    "    parts = instrument.partitionByInstrument(midi)\n",
    "    melody = []\n",
    "    chords = []\n",
    "\n",
    "    if parts: \n",
    "        part = parts.parts[0]\n",
    "    else:\n",
    "        part = midi.flat.notes\n",
    "\n",
    "    for el in part.flat.notesAndRests:\n",
    "        if isinstance(el, note.Note):\n",
    "            melody.append(str(el.pitch))\n",
    "            chords.append(\"C\")  # Placeholder chord\n",
    "        elif isinstance(el, chord.Chord):\n",
    "            melody.append(str(el.root()))\n",
    "            chords.append(str(el.commonName))\n",
    "        elif isinstance(el, note.Rest):\n",
    "            melody.append(\"Rest\")\n",
    "            chords.append(\"C\")\n",
    "\n",
    "    return melody, chords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316af91-aa24-40cb-b72d-8ac3af185535",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "\n",
    "- Iterates through all MIDI files in the dataset.\n",
    "- Parses and collects melodies and chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c483b1f3-c70d-46d6-a95c-a4d6cf40633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/music21/stream/base.py:3675: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/music21/stream/base.py:3675: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n",
      "In /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle: .flat is deprecated.  Call .flatten() instead\n",
      "In /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/mpl-data/stylelib/seaborn-v0_8-bright.mplstyle: .flat is deprecated.  Call .flatten() instead\n",
      "In /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/mpl-data/stylelib/seaborn-v0_8-muted.mplstyle: .flat is deprecated.  Call .flatten() instead\n",
      "In /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle: .flat is deprecated.  Call .flatten() instead\n",
      "In /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/mpl-data/stylelib/_mpl-gallery.mplstyle: .flat is deprecated.  Call .flatten() instead\n",
      "In /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/mpl-data/stylelib/seaborn-v0_8-dark.mplstyle: .flat is deprecated.  Call .flatten() instead\n",
      "In /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/mpl-data/stylelib/seaborn-v0_8-ticks.mplstyle: .flat is deprecated.  Call .flatten() instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/music21/stream/base.py:3675: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Load Dataset\n",
    "# -----------------------------\n",
    "all_melodies = []\n",
    "all_chords = []\n",
    "\n",
    "for fname in os.listdir(DATA_DIR):\n",
    "    if fname.endswith(\".mid\") or fname.endswith(\".midi\"):\n",
    "        try:\n",
    "            mel, chd = parse_midi_file(os.path.join(DATA_DIR, fname))\n",
    "            if len(mel) > 0:\n",
    "                all_melodies.append(mel)\n",
    "                all_chords.append(chd)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {fname}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d45b31-e174-482f-8b6e-ea49b099a673",
   "metadata": {},
   "source": [
    "### Vocabulary Creation\n",
    "\n",
    "- Builds lookup dictionaries for notes and chords.\n",
    "- Each unique token is mapped to an integer index.\n",
    "- Includes \"UNK\" token for unseen notes\n",
    "- Chord vocabulary size typically 10-50 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db68a18-3dcd-4f85-9ee2-4d54f9a49ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Vocab\n",
    "# -----------------------------\n",
    "note_vocab = sorted(list({n for mel in all_melodies for n in mel}))\n",
    "chord_vocab = sorted(list({c for chd in all_chords for c in chd}))\n",
    "\n",
    "note_to_idx = {n: i for i, n in enumerate(note_vocab)}\n",
    "note_to_idx[\"UNK\"] = len(note_to_idx)\n",
    "idx_to_note = {i: n for n, i in note_to_idx.items()}\n",
    "\n",
    "chord_to_idx = {c: i for i, c in enumerate(chord_vocab)}\n",
    "idx_to_chord = {i: c for c, i in chord_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787713d-1be0-4342-9d27-5a16bd7b4aaa",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "- Converts melody and chord sequences into index-based tensors.\n",
    "- Maintains sequence alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f3890d1-709e-4b45-9fd7-97a18d7bf199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Prepare Data\n",
    "# -----------------------------\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for mel, chd in zip(all_melodies, all_chords):\n",
    "    x_seq = [note_to_idx.get(n, note_to_idx[\"UNK\"]) for n in mel]\n",
    "    y_seq = [chord_to_idx[c] for c in chd]\n",
    "    if len(x_seq) == len(y_seq):\n",
    "        x_data.append(torch.tensor(x_seq))\n",
    "        y_data.append(torch.tensor(y_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d42a3fea-3172-4d94-992b-e64e1a80c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Train/Test Split\n",
    "# -----------------------------\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25749c2-f4ae-418b-990b-5b8fbc141769",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caf0dc-edbe-4d28-b34e-6ca24314087e",
   "metadata": {},
   "source": [
    "The model, Harmonizer, is a sequence-to-sequence model using an LSTM (Long Short-Term Memory) network. It maps a sequence of note indices to a sequence of chord indices.\n",
    "\n",
    "1. Embedding Layer: Converts note indices into dense vectors (64-dim); learns semantic relationships between notes\n",
    "2. LSTM Layer: Processes sequential embeddings (128 hidden units); Captures temporal dependencies in melodies; Batch-first: (batch, seq_len, features)\n",
    "3. Fully Connected Layer: Predicts the chord class for each time step; Maps LSTM outputs → chord probabilities\n",
    "\n",
    "Parameters:\n",
    "- Embedding dim: 64\n",
    "- Hidden units: 128\n",
    "- Output size: Number of chord classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c75dcba-2c68-465f-b449-bbf7dd080712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Harmonizer(nn.Module):\n",
    "    def __init__(self, num_notes, num_chords, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_notes, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_chords)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "model = Harmonizer(len(note_to_idx), len(chord_to_idx)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21a973-fa21-4e83-8d3e-c3e44413e430",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "- Trains the model using CrossEntropyLoss.\n",
    "- Each sequence is processed individually due to variable lengths.\n",
    "- Optimization: Adam (lr=0.001)\n",
    "- Batch Handling: Processes sequences individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ef8735-daae-4f21-b5e4-6761c67191cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 243.67\n",
      "Epoch 2, Loss: 121.62\n",
      "Epoch 3, Loss: 103.47\n",
      "Epoch 4, Loss: 91.72\n",
      "Epoch 5, Loss: 85.35\n",
      "Epoch 6, Loss: 81.32\n",
      "Epoch 7, Loss: 77.71\n",
      "Epoch 8, Loss: 74.42\n",
      "Epoch 9, Loss: 71.73\n",
      "Epoch 10, Loss: 68.82\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        y = y.unsqueeze(0).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out.view(-1, out.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89c2ac-1486-4340-8221-42f199e0fea6",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- Reports per-chord class metrics\n",
    "- Handles class imbalance with zero_division=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c64118b-76a9-47df-b6ea-70e05ff5f09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 96.79%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            y = y.unsqueeze(0).to(device)\n",
    "            out = model(x)\n",
    "            pred = out.argmax(dim=-1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"\\nTest Accuracy: {acc:.2f}%\")\n",
    "\n",
    "evaluate(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0dd58ac-727f-4804-b8ea-0355df3565f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                                        C       0.99      1.00      1.00     38914\n",
      "                             Major Second       0.00      0.00      0.00         4\n",
      "                              Major Sixth       0.00      0.00      0.00        23\n",
      "                              Major Third       0.00      0.00      0.00        80\n",
      "                              Minor Sixth       0.00      0.00      0.00        37\n",
      "                              Minor Third       0.00      0.00      0.00        92\n",
      "                            Perfect Fifth       0.00      0.00      0.00         2\n",
      "                           Perfect Fourth       0.00      0.00      0.00         6\n",
      "                   dominant seventh chord       0.68      0.83      0.74      2116\n",
      "enharmonic equivalent to diminished triad       0.60      0.33      0.43         9\n",
      "     enharmonic equivalent to major triad       0.50      0.86      0.63         7\n",
      "     enharmonic to dominant seventh chord       1.00      1.00      1.00        76\n",
      "            half-diminished seventh chord       0.00      0.00      0.00         2\n",
      "        incomplete dominant-seventh chord       0.00      0.00      0.00        36\n",
      "           incomplete minor-seventh chord       0.00      0.00      0.00         1\n",
      "                              major triad       0.95      0.90      0.92      6269\n",
      "                      minor seventh chord       0.00      0.00      0.00        13\n",
      "                              minor triad       0.91      0.84      0.87      1801\n",
      "                                   unison       0.00      0.00      0.00         2\n",
      "\n",
      "                                 accuracy                           0.97     49490\n",
      "                                macro avg       0.30      0.30      0.29     49490\n",
      "                             weighted avg       0.96      0.97      0.97     49490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_with_metrics(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            out = model(x)\n",
    "            preds = out.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "            targets = y.squeeze().tolist()\n",
    "\n",
    "            # Ensure list format\n",
    "            if isinstance(preds, int):\n",
    "                preds = [preds]\n",
    "            if isinstance(targets, int):\n",
    "                targets = [targets]\n",
    "\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(targets)\n",
    "\n",
    "    # Compute only over classes actually present in y_true and y_pred\n",
    "    all_labels = sorted(set(y_true) | set(y_pred))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        y_true, y_pred,\n",
    "        labels=all_labels,\n",
    "        target_names=[idx_to_chord[i] for i in all_labels],\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "evaluate_with_metrics(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e945fd-add6-46b2-9e43-af1e6a8ec5ab",
   "metadata": {},
   "source": [
    "## Harmoinzation\n",
    "- Real-time chord prediction for new melodies\n",
    "- Handles unknown notes via \"UNK\" \n",
    "- Takes a new melody (list of note strings).\n",
    "- Converts it to indices, feeds into the model, and returns predicted chords.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79be9e61-cab0-4974-8e60-4d045727b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Harmonize New Melody\n",
    "# -----------------------------\n",
    "def harmonize(melody_notes):\n",
    "    model.eval()\n",
    "    input_idxs = [note_to_idx.get(n, note_to_idx[\"UNK\"]) for n in melody_notes]\n",
    "    x = torch.tensor(input_idxs).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        pred_idxs = pred.argmax(-1).squeeze().tolist()\n",
    "    return [idx_to_chord[i] for i in pred_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f592a2c-3fd5-430f-851b-f02ef7b45b74",
   "metadata": {},
   "source": [
    "### Save to MIDI\n",
    "- Uses music21 to create a MIDI file from melody and chords.\n",
    "- Currently only saves melody notes (not chord sounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d055c694-90c7-4a2c-bc7f-e61b71397649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Save Harmonized MIDI\n",
    "# -----------------------------\n",
    "def save_to_midi(melody, chords, filename=\"harmonized.mid\"):\n",
    "    s = stream.Stream()\n",
    "    for m, c in zip(melody, chords):\n",
    "        n = note.Note(m) if m != \"Rest\" else note.Rest()\n",
    "        n.quarterLength = 1.0\n",
    "        s.append(n)\n",
    "    s.write(\"midi\", fp=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f8bcf-4597-48f3-b9ba-c67bf2f39f85",
   "metadata": {},
   "source": [
    "## Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e02dab2-d66a-4349-b9ea-d956773848d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melody: ['Rest', 'Rest', 'G5', 'F#5', 'E5', 'A2', 'A4', 'A4', 'A4', 'A4', 'E5', 'A4', 'C5', 'A4', 'B4', 'E2', 'D5', 'E5', 'F#5', 'G5', 'D2', 'E5', 'D5', 'B4', 'G4', 'G2', 'B4', 'G4', 'D5', 'G4', 'B4', 'G4', 'B4', 'G2', 'D5', 'E5', 'F#5', 'G5', 'E2', 'F#5', 'G5', 'E5', 'A2', 'A4', 'A4', 'A4', 'A4', 'E5', 'F#5', 'E5', 'D5', 'C#5', 'A2', 'D5', 'E5', 'F#5', 'G5', 'E2', 'F#5', 'G5', 'B5', 'A5', 'D2', 'F#5', 'G5', 'E5', 'D5', 'G2', 'B4', 'G4', 'B4', 'A4', 'E2', 'C5', 'B4', 'G4', 'A4', 'A2', 'G5', 'F#5', 'E5', 'A2', 'A4', 'A4', 'A4', 'A4', 'E5', 'A4', 'C5', 'A4', 'B4', 'E2', 'D5', 'E5', 'F#5', 'G5', 'D2', 'E5', 'D5', 'B4']\n",
      "Predicted Chords: ['C', 'C', 'C', 'C', 'C', 'major triad', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'dominant seventh chord', 'C', 'C', 'C', 'C', 'dominant seventh chord', 'C', 'C', 'C', 'C', 'major triad', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'major triad', 'C', 'C', 'C', 'C', 'minor triad', 'C', 'C', 'C', 'minor triad', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'dominant seventh chord', 'C', 'C', 'C', 'C', 'minor triad', 'C', 'C', 'C', 'C', 'major triad', 'C', 'C', 'C', 'C', 'major triad', 'C', 'C', 'C', 'C', 'minor triad', 'C', 'C', 'C', 'C', 'minor triad', 'C', 'C', 'C', 'minor triad', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'minor triad', 'C', 'C', 'C', 'C', 'dominant seventh chord', 'C', 'C', 'C']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Example Run\n",
    "# -----------------------------\n",
    "input_melody = all_melodies[0][:100]\n",
    "predicted_chords = harmonize(input_melody)\n",
    "print(\"Melody:\", input_melody)\n",
    "print(\"Predicted Chords:\", predicted_chords)\n",
    "save_to_midi(input_melody, predicted_chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd3657b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
