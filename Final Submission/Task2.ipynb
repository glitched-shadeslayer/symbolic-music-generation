{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2f965e-1c7b-4827-948e-012dc07bf072",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e0304-69eb-49ba-8add-2398d0246a22",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84639806-a39c-4e5b-905b-8fcb671b498d",
   "metadata": {},
   "source": [
    "This project focuses on symbolic music harmonization, a task in the field of music information retrieval (MIR) and AI-generated music. The goal is to predict a sequence of chords that complement a given melody, using symbolic representations of music—typically notes and chords encoded from MIDI files. This falls under the broader umbrella of conditioned generation, where one musical element (melody) is used to condition the generation of another (harmony)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f849e3-bedc-427d-9430-9524777fac60",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd644796-0331-4654-9169-e7ff482d1630",
   "metadata": {},
   "source": [
    "Given a sequence of musical notes (melody), automatically generate a corresponding sequence of chords (harmony) that musically complements the melody. This mirrors tasks a human composer might perform and is especially useful for music composition assistance, educational tools, and interactive music applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f422b12-456f-4af7-90c6-95bd377ee8d2",
   "metadata": {},
   "source": [
    "## Symbolic Conditioned Generation & Harmoization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b0953-c282-4993-876a-dd836a6291bd",
   "metadata": {},
   "source": [
    "Symbolic refers to data formats like MIDI, which encode music as discrete symbolic events (notes, durations, velocities) rather than audio waveforms. Unlike audio signals, symbolic representations allow for structured and interpretable data manipulation. The model generates chord sequences conditioned on input melodies, creating musically relevant outputs based on specific inputs. This makes the task similar to machine translation in NLP, where an input sentence (melody) is \"translated\" into another sequence (chords). Harmonization is the process of adding chords to a melody according to music theory principles (e.g., chord functions, voice leading) - creating a fuller musical texture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc96dace-64e3-42a0-b26e-50e7a095396f",
   "metadata": {},
   "source": [
    "## Dataset: Nottingham MIDI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98018308-3ff8-47d1-b576-c842508d13ae",
   "metadata": {},
   "source": [
    "Source:\n",
    "Collection of 1,200+ folk tunes in MIDI format from the Nottingham Music Database. The dataset contains folk tunes each with a melody and a corresponding harmony. \n",
    "\n",
    "Characteristics:\n",
    "1. Primarily monophonic melodies with chord annotations\n",
    "2. Contains folk, dance, and ballad styles\n",
    "3. Typical structure: 8-32 bar phrases in 4/4 time\n",
    "\n",
    "Each file is parsed to extract:\n",
    "1. Melody: A sequence of notes or rests.\n",
    "2. Chords: Either actual chords from the file or placeholder labels (in this code, placeholder \"C\" is used).\n",
    "\n",
    "Format: Standard MIDI files, parsed using music21\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee749dd6-b697-4bd0-a4cc-60490f1bf4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting music21\n",
      "  Using cached music21-9.7.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting chardet (from music21)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\nush\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from music21) (1.4.2)\n",
      "Collecting jsonpickle (from music21)\n",
      "  Using cached jsonpickle-4.1.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nush\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from music21) (3.10.0)\n",
      "Collecting more-itertools (from music21)\n",
      "  Using cached more_itertools-10.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting numpy<2.0.0 (from music21)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [21 lines of output]\n",
      "  + C:\\Users\\Nush\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\Nush\\AppData\\Local\\Temp\\pip-install-radgrxnp\\numpy_9a25f3fe41a542d1ab5837a46bd9f41c\\vendored-meson\\meson\\meson.py setup C:\\Users\\Nush\\AppData\\Local\\Temp\\pip-install-radgrxnp\\numpy_9a25f3fe41a542d1ab5837a46bd9f41c C:\\Users\\Nush\\AppData\\Local\\Temp\\pip-install-radgrxnp\\numpy_9a25f3fe41a542d1ab5837a46bd9f41c\\.mesonpy-c0ago6ov -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\Nush\\AppData\\Local\\Temp\\pip-install-radgrxnp\\numpy_9a25f3fe41a542d1ab5837a46bd9f41c\\.mesonpy-c0ago6ov\\meson-python-native-file.ini\n",
      "  The Meson build system\n",
      "  Version: 1.2.99\n",
      "  Source dir: C:\\Users\\Nush\\AppData\\Local\\Temp\\pip-install-radgrxnp\\numpy_9a25f3fe41a542d1ab5837a46bd9f41c\n",
      "  Build dir: C:\\Users\\Nush\\AppData\\Local\\Temp\\pip-install-radgrxnp\\numpy_9a25f3fe41a542d1ab5837a46bd9f41c\\.mesonpy-c0ago6ov\n",
      "  Build type: native build\n",
      "  Project name: NumPy\n",
      "  Project version: 1.26.4\n",
      "  WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "  \n",
      "  ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "  The following exception(s) were encountered:\n",
      "  Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  \n",
      "  A full log can be found at C:\\Users\\Nush\\AppData\\Local\\Temp\\pip-install-radgrxnp\\numpy_9a25f3fe41a542d1ab5837a46bd9f41c\\.mesonpy-c0ago6ov\\meson-logs\\meson-log.txt\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c4b432-1b55-4054-bc12-6d0451832132",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'music21'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmusic21\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m converter, instrument, note, chord, stream\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'music21'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c357d4-d46b-44f0-8d1f-bd019b5af5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"nottingham-dataset-master/MIDI/\"  # Update to your MIDI folder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Sets data path and computation device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352af33-8e8d-480a-a321-a799cfa59cc2",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290876d-403a-4fae-9d6f-4e8c64170ba2",
   "metadata": {},
   "source": [
    "File Parsing: Extract notes and chords using music21\n",
    "\n",
    "Simplification:\n",
    "\n",
    "1. Melodies: Convert to pitch strings (e.g., \"C4\") or \"Rest\"\n",
    "\n",
    "2. Chords: Simplified to root note names (original code uses placeholder)\n",
    "\n",
    "Sequence Alignment: Ensure 1:1 correspondence between notes and chords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e442af8-0b77-48d4-b43b-e846377adad2",
   "metadata": {},
   "source": [
    "### MIDI Parsing\n",
    "- Uses music21 to parse MIDI files.\n",
    "- Extracts melodies and (placeholder) chords from each file.\n",
    "- Converts musical elements to string-based symbolic representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59bd39-7a8c-4844-82ba-d70a46921039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# MIDI Parser\n",
    "# -----------------------------\n",
    "def parse_midi_file(file_path):\n",
    "    midi = converter.parse(file_path)\n",
    "    parts = instrument.partitionByInstrument(midi)\n",
    "    melody = []\n",
    "    chords = []\n",
    "\n",
    "    if parts: \n",
    "        part = parts.parts[0]\n",
    "    else:\n",
    "        part = midi.flat.notes\n",
    "\n",
    "    for el in part.flat.notesAndRests:\n",
    "        if isinstance(el, note.Note):\n",
    "            melody.append(str(el.pitch))\n",
    "            chords.append(\"C\")  # Placeholder chord\n",
    "        elif isinstance(el, chord.Chord):\n",
    "            melody.append(str(el.root()))\n",
    "            chords.append(str(el.commonName))\n",
    "        elif isinstance(el, note.Rest):\n",
    "            melody.append(\"Rest\")\n",
    "            chords.append(\"C\")\n",
    "\n",
    "    return melody, chords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316af91-aa24-40cb-b72d-8ac3af185535",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "\n",
    "- Iterates through all MIDI files in the dataset.\n",
    "- Parses and collects melodies and chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483b1f3-c70d-46d6-a95c-a4d6cf40633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load Dataset\n",
    "# -----------------------------\n",
    "all_melodies = []\n",
    "all_chords = []\n",
    "\n",
    "for fname in os.listdir(DATA_DIR):\n",
    "    if fname.endswith(\".mid\") or fname.endswith(\".midi\"):\n",
    "        try:\n",
    "            mel, chd = parse_midi_file(os.path.join(DATA_DIR, fname))\n",
    "            if len(mel) > 0:\n",
    "                all_melodies.append(mel)\n",
    "                all_chords.append(chd)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {fname}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d45b31-e174-482f-8b6e-ea49b099a673",
   "metadata": {},
   "source": [
    "### Vocabulary Creation\n",
    "\n",
    "- Builds lookup dictionaries for notes and chords.\n",
    "- Each unique token is mapped to an integer index.\n",
    "- Includes \"UNK\" token for unseen notes\n",
    "- Chord vocabulary size typically 10-50 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db68a18-3dcd-4f85-9ee2-4d54f9a49ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Vocab\n",
    "# -----------------------------\n",
    "note_vocab = sorted(list({n for mel in all_melodies for n in mel}))\n",
    "chord_vocab = sorted(list({c for chd in all_chords for c in chd}))\n",
    "\n",
    "note_to_idx = {n: i for i, n in enumerate(note_vocab)}\n",
    "note_to_idx[\"UNK\"] = len(note_to_idx)\n",
    "idx_to_note = {i: n for n, i in note_to_idx.items()}\n",
    "\n",
    "chord_to_idx = {c: i for i, c in enumerate(chord_vocab)}\n",
    "idx_to_chord = {i: c for c, i in chord_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787713d-1be0-4342-9d27-5a16bd7b4aaa",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "- Converts melody and chord sequences into index-based tensors.\n",
    "- Maintains sequence alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3890d1-709e-4b45-9fd7-97a18d7bf199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Prepare Data\n",
    "# -----------------------------\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for mel, chd in zip(all_melodies, all_chords):\n",
    "    x_seq = [note_to_idx.get(n, note_to_idx[\"UNK\"]) for n in mel]\n",
    "    y_seq = [chord_to_idx[c] for c in chd]\n",
    "    if len(x_seq) == len(y_seq):\n",
    "        x_data.append(torch.tensor(x_seq))\n",
    "        y_data.append(torch.tensor(y_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d42a3fea-3172-4d94-992b-e64e1a80c876",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Train/Test Split\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(x_data, y_data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Train/Test Split\n",
    "# -----------------------------\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25749c2-f4ae-418b-990b-5b8fbc141769",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caf0dc-edbe-4d28-b34e-6ca24314087e",
   "metadata": {},
   "source": [
    "The model, Harmonizer, is a sequence-to-sequence model using an LSTM (Long Short-Term Memory) network. It maps a sequence of note indices to a sequence of chord indices.\n",
    "\n",
    "1. Embedding Layer: Converts note indices into dense vectors (64-dim); learns semantic relationships between notes\n",
    "2. LSTM Layer: Processes sequential embeddings (128 hidden units); Captures temporal dependencies in melodies; Batch-first: (batch, seq_len, features)\n",
    "3. Fully Connected Layer: Predicts the chord class for each time step; Maps LSTM outputs → chord probabilities\n",
    "\n",
    "Parameters:\n",
    "- Embedding dim: 64\n",
    "- Hidden units: 128\n",
    "- Output size: Number of chord classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c75dcba-2c68-465f-b449-bbf7dd080712",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mHarmonizer\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_notes, num_chords, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Harmonizer(nn.Module):\n",
    "    def __init__(self, num_notes, num_chords, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_notes, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_chords)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "model = Harmonizer(len(note_to_idx), len(chord_to_idx)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21a973-fa21-4e83-8d3e-c3e44413e430",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "- Trains the model using CrossEntropyLoss.\n",
    "- Each sequence is processed individually due to variable lengths.\n",
    "- Optimization: Adam (lr=0.001)\n",
    "- Batch Handling: Processes sequences individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef8735-daae-4f21-b5e4-6761c67191cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        y = y.unsqueeze(0).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out.view(-1, out.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89c2ac-1486-4340-8221-42f199e0fea6",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- Reports per-chord class metrics\n",
    "- Handles class imbalance with zero_division=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64118b-76a9-47df-b6ea-70e05ff5f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            y = y.unsqueeze(0).to(device)\n",
    "            out = model(x)\n",
    "            pred = out.argmax(dim=-1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"\\nTest Accuracy: {acc:.2f}%\")\n",
    "\n",
    "evaluate(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd58ac-727f-4804-b8ea-0355df3565f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_metrics(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            out = model(x)\n",
    "            preds = out.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "            targets = y.squeeze().tolist()\n",
    "\n",
    "            # Ensure list format\n",
    "            if isinstance(preds, int):\n",
    "                preds = [preds]\n",
    "            if isinstance(targets, int):\n",
    "                targets = [targets]\n",
    "\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(targets)\n",
    "\n",
    "    # Compute only over classes actually present in y_true and y_pred\n",
    "    all_labels = sorted(set(y_true) | set(y_pred))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        y_true, y_pred,\n",
    "        labels=all_labels,\n",
    "        target_names=[idx_to_chord[i] for i in all_labels],\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "evaluate_with_metrics(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e945fd-add6-46b2-9e43-af1e6a8ec5ab",
   "metadata": {},
   "source": [
    "## Harmoinzation\n",
    "- Real-time chord prediction for new melodies\n",
    "- Handles unknown notes via \"UNK\" \n",
    "- Takes a new melody (list of note strings).\n",
    "- Converts it to indices, feeds into the model, and returns predicted chords.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be9e61-cab0-4974-8e60-4d045727b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Harmonize New Melody\n",
    "# -----------------------------\n",
    "def harmonize(melody_notes):\n",
    "    model.eval()\n",
    "    input_idxs = [note_to_idx.get(n, note_to_idx[\"UNK\"]) for n in melody_notes]\n",
    "    x = torch.tensor(input_idxs).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        pred_idxs = pred.argmax(-1).squeeze().tolist()\n",
    "    return [idx_to_chord[i] for i in pred_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f592a2c-3fd5-430f-851b-f02ef7b45b74",
   "metadata": {},
   "source": [
    "### Save to MIDI\n",
    "- Uses music21 to create a MIDI file from melody and chords.\n",
    "- Currently only saves melody notes (not chord sounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055c694-90c7-4a2c-bc7f-e61b71397649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Save Harmonized MIDI\n",
    "# -----------------------------\n",
    "def save_to_midi(melody, chords, filename=\"harmonized.mid\"):\n",
    "    s = stream.Stream()\n",
    "    for m, c in zip(melody, chords):\n",
    "        n = note.Note(m) if m != \"Rest\" else note.Rest()\n",
    "        n.quarterLength = 1.0\n",
    "        s.append(n)\n",
    "    s.write(\"midi\", fp=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f8bcf-4597-48f3-b9ba-c67bf2f39f85",
   "metadata": {},
   "source": [
    "## Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02dab2-d66a-4349-b9ea-d956773848d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Example Run\n",
    "# -----------------------------\n",
    "input_melody = all_melodies[0][:30]\n",
    "predicted_chords = harmonize(input_melody)\n",
    "print(\"Melody:\", input_melody)\n",
    "print(\"Predicted Chords:\", predicted_chords)\n",
    "save_to_midi(input_melody, predicted_chords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
